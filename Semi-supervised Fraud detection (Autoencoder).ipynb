{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is from Kaggle for the use case of fraud detection in credit card transactions. Columns are pretty generic, but all values are type float. Can imagine other manipulations were made to this dataset such as converting categorical data to numeric for the purpose of feeding into the auto encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit Card Fraud Data\n",
    "# https://www.kaggle.com/code/shivamb/semi-supervised-classification-using-autoencoders/data\n",
    "df = pd.read_csv(r'C:\\Users\\Aaron\\.kaggle\\creditcard.csv\\creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>284315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count\n",
       "Class        \n",
       "0      284315\n",
       "1         492"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 = Non Fraud, 1 = Fraud \n",
    "pd.DataFrame(df.groupby('Class').size()).rename(columns={0:'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.919560e-15</td>\n",
       "      <td>5.688174e-16</td>\n",
       "      <td>-8.769071e-15</td>\n",
       "      <td>2.782312e-15</td>\n",
       "      <td>-1.552563e-15</td>\n",
       "      <td>2.010663e-15</td>\n",
       "      <td>-1.694249e-15</td>\n",
       "      <td>-1.927028e-16</td>\n",
       "      <td>-3.137024e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537294e-16</td>\n",
       "      <td>7.959909e-16</td>\n",
       "      <td>5.367590e-16</td>\n",
       "      <td>4.458112e-15</td>\n",
       "      <td>1.453003e-15</td>\n",
       "      <td>1.699104e-15</td>\n",
       "      <td>-3.660161e-16</td>\n",
       "      <td>-1.206049e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.537294e-16  7.959909e-16  5.367590e-16  4.458112e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   1.453003e-15  1.699104e-15 -3.660161e-16 -1.206049e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table Stats\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Data for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the semi-supervised aspect takes place. The \"non-fraud\" rows are separated from the fraud rows and each are respectively put into their own datasets. We will train the model on the non-fraud rows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Data and drop class labels\n",
    "nonFraud = df[df['Class'] == 0].drop('Class',axis=1)\n",
    "fraud = df[df['Class'] == 1].drop('Class',axis=1)\n",
    "\n",
    "# Train Test Split for nonFraud Auto Encoder training\n",
    "x_train, x_test = train_test_split(nonFraud,test_size=.35, random_state = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build & Fit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and fitting the auto encoder model. Both encoding and decoding layers consists of 25 neurons and the hidden layer contains 3. Essentially the model will compress the input data down into this 3 neuron latent representation of the data and then rebuild the data using the learned latent representation. Then in an iterative process we will train the model to reduce the difference from the input data and the output data built from the hidden layer. Our loss is based on the mean squared error and we trained the model for 50 iterations/epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5776/5776 [==============================] - 10s 2ms/step - loss: 3599354.5000\n",
      "Epoch 2/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 350.3138\n",
      "Epoch 3/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 423.5440\n",
      "Epoch 4/50\n",
      "5776/5776 [==============================] - 10s 2ms/step - loss: 339.6102\n",
      "Epoch 5/50\n",
      "5776/5776 [==============================] - 10s 2ms/step - loss: 359.6619\n",
      "Epoch 6/50\n",
      "5776/5776 [==============================] - 10s 2ms/step - loss: 318.4827\n",
      "Epoch 7/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 399.5427\n",
      "Epoch 8/50\n",
      "5776/5776 [==============================] - 10s 2ms/step - loss: 273.5452\n",
      "Epoch 9/50\n",
      "5776/5776 [==============================] - 10s 2ms/step - loss: 251.4909\n",
      "Epoch 10/50\n",
      "5776/5776 [==============================] - 10s 2ms/step - loss: 265.1780\n",
      "Epoch 11/50\n",
      "5776/5776 [==============================] - 10s 2ms/step - loss: 324.2635\n",
      "Epoch 12/50\n",
      "5776/5776 [==============================] - 10s 2ms/step - loss: 265.6183\n",
      "Epoch 13/50\n",
      "5776/5776 [==============================] - 10s 2ms/step - loss: 270.8491\n",
      "Epoch 14/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 244.1614\n",
      "Epoch 15/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 250.1205\n",
      "Epoch 16/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 234.3713\n",
      "Epoch 17/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 207.2388\n",
      "Epoch 18/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 221.9874\n",
      "Epoch 19/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 219.0800\n",
      "Epoch 20/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 224.7753\n",
      "Epoch 21/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 198.7117\n",
      "Epoch 22/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 222.3119\n",
      "Epoch 23/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 203.2349\n",
      "Epoch 24/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 197.5854A: 0s\n",
      "Epoch 25/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 198.4889\n",
      "Epoch 26/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 194.6195\n",
      "Epoch 27/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 217.3569\n",
      "Epoch 28/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 184.8155\n",
      "Epoch 29/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 208.9260\n",
      "Epoch 30/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 157.1627\n",
      "Epoch 31/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 203.7513\n",
      "Epoch 32/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 174.9657A\n",
      "Epoch 33/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 162.0656\n",
      "Epoch 34/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 165.2071\n",
      "Epoch 35/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 169.6622\n",
      "Epoch 36/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 166.9787\n",
      "Epoch 37/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 147.5990\n",
      "Epoch 38/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 159.6402\n",
      "Epoch 39/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 173.0193\n",
      "Epoch 40/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 133.1058\n",
      "Epoch 41/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 157.2712\n",
      "Epoch 42/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 173.7444\n",
      "Epoch 43/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 142.1316\n",
      "Epoch 44/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 144.1694\n",
      "Epoch 45/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 133.8601\n",
      "Epoch 46/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 135.8505\n",
      "Epoch 47/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 126.5747A\n",
      "Epoch 48/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 133.7658A: 0s - loss\n",
      "Epoch 49/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 154.8180\n",
      "Epoch 50/50\n",
      "5776/5776 [==============================] - 9s 2ms/step - loss: 137.1181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26a260a3ac0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Auto Encoder\n",
    "\n",
    "modelAE = models.Sequential([\n",
    "    layers.InputLayer(input_shape= x_train.shape[1]),\n",
    "    layers.Dense(25,activation='relu'), # encoding layer\n",
    "    layers.Dense(3,activation='relu'), # latent hidden layer\n",
    "    layers.Dense(25,activation='relu'), # decoding layer\n",
    "    layers.Dense(x_train.shape[1])\n",
    "])\n",
    "\n",
    "modelAE.compile(loss='mean_squared_error',optimizer='adam')\n",
    "modelAE.fit(x_train,x_train,verbose=1,epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of Sample Non-Fraud (RMSE): 2.025225702585351\n",
      "Insample Non-Fraud Score (RMSE): 2.0763803448145226\n",
      "Fraud Score (RMSE): 5.405681401914101\n"
     ]
    }
   ],
   "source": [
    "# How does the model perform on new non-fraud data\n",
    "pred = modelAE.predict(x_test)\n",
    "score1 = np.sqrt(metrics.mean_squared_error(pred,x_test))\n",
    "\n",
    "# How does the model perform on all non-fraud data\n",
    "pred = modelAE.predict(nonFraud)\n",
    "score2 = np.sqrt(metrics.mean_squared_error(pred,nonFraud))\n",
    "\n",
    "# How does the model perform on fraud data\n",
    "pred = modelAE.predict(fraud)\n",
    "score3 = np.sqrt(metrics.mean_squared_error(pred,fraud))\n",
    "print(f\"Out of Sample Non-Fraud (RMSE): {score1}\")\n",
    "print(f\"Insample Non-Fraud Score (RMSE): {score2}\")\n",
    "print(f\"Fraud Score (RMSE): {score3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the results that the errors associated with the insample and out of sample non-fraud data are about the same. This means, using the hidden latent representation determined by the training data, the non-fraud data fed through is about the same producing about the same errors. However, the fraud data produces a greater error meaning that our hidden representation could not predict the fraud data as accuractely eluding to the fact that there is something different about this underlying data in comparison to the data that was used for training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
